{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6044d56f",
   "metadata": {},
   "source": [
    "# Lichen Likelihood Project: Expectation Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "33cb1520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cd5f9173",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_df = pd.read_pickle('element_analysis.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2eeb8158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Nitrogen (% dw)_binned', 'Sulfur (% dw)_binned',\n",
       "       'Phosphorous (ppm dw)_binned', 'Lead (ppm dw)_binned',\n",
       "       'Copper (ppm dw)_binned', 'Chromium (ppm dw)_binned',\n",
       "       'Air pollution score_binned', 'Year of tissue collection_binned',\n",
       "       'Region_binned',\n",
       "       'Code for scientific name and authority in lookup table_binned'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print columns ending in \"binned\"\n",
    "unpickled_df.columns[unpickled_df.columns.str.endswith('binned')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a631ef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "df = unpickled_df.copy()\n",
    "def idx_map_from_series(s):\n",
    "    \"\"\"Return dict mapping original category\"\"\"\n",
    "    cats = list(pd.Categorical(s).categories)\n",
    "    mapping = {c:i for i,c in enumerate(cats)}\n",
    "    return mapping, cats\n",
    "\n",
    "def one_hot_index_from_value(mapping, val):\n",
    "    return mapping[val]\n",
    "\n",
    "def normalize_rows(mat, axis=-1, alpha=1e-2):\n",
    "    \"\"\"\n",
    "    Normalize\n",
    "    \"\"\"\n",
    "    arr = mat.astype(float).copy()\n",
    "    arr += float(alpha)\n",
    "    s = arr.sum(axis=axis, keepdims=True)\n",
    "    s[s == 0] = 1.0\n",
    "    return arr / s\n",
    "\n",
    "def logsumexp(a, axis=None): # self explanatory\n",
    "    a_max = np.max(a, axis=axis, keepdims=True)\n",
    "    res = a_max + np.log(np.sum(np.exp(a - a_max), axis=axis, keepdims=True))\n",
    "    if axis is not None:\n",
    "        return np.squeeze(res, axis=axis)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bb62ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-10\n",
    "class BayesianNetworkEM:\n",
    "    def __init__(self, df, hidden_states=2, seed=0):\n",
    "        \"\"\"\n",
    "        Initializes CPT tables with random values consistent with the BN structure.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.N = len(self.df)\n",
    "        self.H = hidden_states\n",
    "        # element node column names\n",
    "        self.elements = [\n",
    "            'Nitrogen (% dw)_binned',\n",
    "            'Sulfur (% dw)_binned',\n",
    "            'Phosphorous (ppm dw)_binned',\n",
    "            'Lead (ppm dw)_binned',\n",
    "            'Copper (ppm dw)_binned',\n",
    "            'Chromium (ppm dw)_binned'\n",
    "        ]\n",
    "        self.year_col = 'Year of tissue collection_binned'\n",
    "        self.pollution_col = 'Air pollution score_binned'\n",
    "        self.region_col = 'Region_binned'\n",
    "        self.species_col = 'Code for scientific name and authority in lookup table_binned'\n",
    "        self.maps = {}\n",
    "        self.rev = {}\n",
    "        cols_for_map = self.elements + [self.year_col, self.pollution_col, self.region_col, self.species_col]\n",
    "        for col in cols_for_map:\n",
    "            mapping, cats = idx_map_from_series(self.df[col])\n",
    "            self.maps[col] = mapping\n",
    "            self.rev[col] = cats\n",
    "\n",
    "        self.K_region = len(self.rev[self.region_col])\n",
    "        self.K_year = len(self.rev[self.year_col])\n",
    "        self.K_poll = len(self.rev[self.pollution_col])\n",
    "        self.K_species = len(self.rev[self.species_col])\n",
    "        self.K_elem = {col: len(self.rev[col]) for col in self.elements}\n",
    "        self.initialize_random_CPTs()\n",
    "        self.gamma = np.zeros((self.N, self.H))\n",
    "\n",
    "    def initialize_random_CPTs(self):\n",
    "        \"\"\"\n",
    "        Randomly initialize all CPTs:\n",
    "        Ensure each CPT is normalized over its conditional domain.\n",
    "        \"\"\"\n",
    "        self.P_H_given_RY = normalize_rows(np.random.rand(self.K_region, self.K_year, self.H), axis=2)\n",
    "\n",
    "        self.P_Pe_given_H = normalize_rows(np.random.rand(self.H, self.K_poll), axis=1)\n",
    "\n",
    "        # model P(Sp | H, Pe)\n",
    "        self.P_Sp_given_HPe = normalize_rows(\n",
    "            np.random.rand(self.H, self.K_poll, self.K_species), axis=2\n",
    "        )\n",
    "\n",
    "        # elements depend on (Pe, Sp)\n",
    "        self.P_elem = {}\n",
    "        for col in self.elements:\n",
    "            arr = np.random.rand(self.K_poll, self.K_species, self.K_elem[col])\n",
    "            self.P_elem[col] = normalize_rows(arr, axis=2)\n",
    "\n",
    "    def e_step(self):\n",
    "        \"\"\"\n",
    "        Performs the E-step\n",
    "            For each sample n:\n",
    "                Compute posterior responsibilities\n",
    "            Store in self.gamma with shape [num_samples, num_hidden_states]\n",
    "        \"\"\"\n",
    "        idx_region = self.df[self.region_col].map(self.maps[self.region_col]).values\n",
    "        idx_year = self.df[self.year_col].map(self.maps[self.year_col]).values\n",
    "        idx_species = self.df[self.species_col].map(self.maps[self.species_col]).values\n",
    "        idx_poll = self.df[self.pollution_col].map(self.maps[self.pollution_col]).values\n",
    "\n",
    "        N = self.N\n",
    "        H = self.H\n",
    "        Kp = self.K_poll\n",
    "\n",
    "        for i in range(N):\n",
    "            r = idx_region[i]\n",
    "            y = idx_year[i]\n",
    "            sp = idx_species[i]\n",
    "            p_val = idx_poll[i]\n",
    "            # log P(H | r,y)\n",
    "            log_ph = np.log(self.P_H_given_RY[r, y, :] + EPS)\n",
    "            # log P(Pe | H)\n",
    "            log_ppe = np.log(self.P_Pe_given_H[:, p_val] + EPS)\n",
    "            # log P(Sp | H, Pe)\n",
    "            log_psp = np.log(self.P_Sp_given_HPe[:, p_val, sp] + EPS)\n",
    "            # elements are d seperated from H when Pe and Sp are observed\n",
    "            log_unnorm = log_ph + log_ppe + log_psp\n",
    "            log_denom = logsumexp(log_unnorm, axis=0)\n",
    "            self.gamma[i, :] = np.exp(log_unnorm - log_denom)\n",
    "\n",
    "        return self.gamma\n",
    "\n",
    "    def compute_posterior_hidden_probs(self, sample_row):\n",
    "        \"\"\"\n",
    "        Compute P(H=h | observed sample features) for a single sample\n",
    "        Returns a vector normalized over hidden states\n",
    "        \"\"\"\n",
    "        r = self.maps[self.region_col][sample_row[self.region_col]]\n",
    "        y = self.maps[self.year_col][sample_row[self.year_col]]\n",
    "        s = self.maps[self.species_col][sample_row[self.species_col]]\n",
    "\n",
    "        log_ph = np.log(self.P_H_given_RY[r, y, :] + EPS)\n",
    "        if self.pollution_col in sample_row.index and pd.notna(sample_row[self.pollution_col]):\n",
    "            p_obs = self.maps[self.pollution_col][sample_row[self.pollution_col]]\n",
    "            log_ppe = np.log(self.P_Pe_given_H[:, p_obs] + EPS)\n",
    "            # include Sp | Pe\n",
    "            log_psp_obs = np.log(self.P_Sp_given_HPe[:, p_obs, s] + EPS)\n",
    "            log_unnorm = log_ph + log_ppe + log_psp_obs\n",
    "            log_norm = logsumexp(log_unnorm, axis=0)\n",
    "            return np.exp(log_unnorm - log_norm)\n",
    "\n",
    "        # log_contrib_pe[h] = log P(Pe=pe | H=h) + log P(Sp=s | H,Pe=pe) + sum_k log P(elem_k | pe, s) forall (updates pe)\n",
    "        Kp = self.K_poll\n",
    "        log_contrib = np.zeros((Kp, self.H)) \n",
    "        elem_vals = {}\n",
    "        for col in self.elements:\n",
    "            elem_vals[col] = self.maps[col][sample_row[col]]\n",
    "        for pe in range(Kp):\n",
    "            # log P(Pe=pe | H)\n",
    "            logppe = np.log(self.P_Pe_given_H[:, pe] + EPS)\n",
    "            # log P(Sp = s | H, Pe=pe)\n",
    "            log_psp_pe = np.log(self.P_Sp_given_HPe[:, pe, s] + EPS)\n",
    "            # sum over log P(elem | pe, s)\n",
    "            log_elems = 0.0\n",
    "            for col in self.elements:\n",
    "                log_elems += np.log(self.P_elem[col][pe, s, elem_vals[col]] + EPS)\n",
    "            # log contribution per h\n",
    "            log_contrib[pe, :] = logppe + log_psp_pe + log_elems\n",
    "\n",
    "        # log_p_h = log_ph + logsum_pe [ logppe + log_psp_pe + log_elems ] (update for each hidden)\n",
    "        log_sum_pe_per_h = logsumexp(log_contrib, axis=0)\n",
    "        log_p_h = log_ph + log_sum_pe_per_h\n",
    "        log_norm = logsumexp(log_p_h, axis=0)\n",
    "        return np.exp(log_p_h - log_norm)\n",
    "\n",
    "    def m_step(self):\n",
    "        \"\"\"\n",
    "        Performs the M-step:\n",
    "            Update CPTs:\n",
    "                - P(H | Region, FieldDate)\n",
    "                - P(Pollution | H)\n",
    "                - P(Species | H, Pollution)\n",
    "                - P(Element_i_bucket | Species, Pollution) for each element node\n",
    "        Uses expected counts\n",
    "        \"\"\"\n",
    "        # precompute indices\n",
    "        idx_region = self.df[self.region_col].map(self.maps[self.region_col]).values\n",
    "        idx_year = self.df[self.year_col].map(self.maps[self.year_col]).values\n",
    "        idx_species = self.df[self.species_col].map(self.maps[self.species_col]).values\n",
    "        idx_poll = self.df[self.pollution_col].map(self.maps[self.pollution_col]).values\n",
    "        N = self.N\n",
    "        H = self.H\n",
    "        Kp = self.K_poll\n",
    "\n",
    "        #P(H | Region, Year) counts\n",
    "        counts_H_RY = np.zeros((self.K_region, self.K_year, H))\n",
    "        for i in range(N):\n",
    "            r = idx_region[i]; y = idx_year[i]\n",
    "            counts_H_RY[r, y, :] += self.gamma[i, :] \n",
    "        self.P_H_given_RY = normalize_rows(counts_H_RY, axis=2)\n",
    "        # P(Pe | H)\n",
    "        counts_Pe_H = np.zeros((H, Kp))\n",
    "        for i in range(N):\n",
    "            p = idx_poll[i]\n",
    "            counts_Pe_H[:, p] += self.gamma[i, :]\n",
    "        # counts_Pe_H \n",
    "        self.P_Pe_given_H = normalize_rows(counts_Pe_H, axis=1)\n",
    "        # P(Sp | H, Pe)\n",
    "        counts_Sp_HPe = np.zeros((H, Kp, self.K_species))\n",
    "        for i in range(N):\n",
    "            p = idx_poll[i]\n",
    "            s = idx_species[i]\n",
    "            counts_Sp_HPe[:, p, s] += self.gamma[i, :]\n",
    "        # normalize over species\n",
    "        self.P_Sp_given_HPe = normalize_rows(counts_Sp_HPe, axis=2)\n",
    "\n",
    "        # 4) P(elem | Pe, Sp) doesnt need gamma bc dsep\n",
    "        for col in self.elements:\n",
    "            Kval = self.K_elem[col]\n",
    "            counts_elem = np.zeros((Kp, self.K_species, Kval))\n",
    "            for i in range(N):\n",
    "                p = idx_poll[i]\n",
    "                s = idx_species[i]\n",
    "                val = self.maps[col][self.df[col].iloc[i]]\n",
    "                counts_elem[p, s, val] += 1.0\n",
    "            self.P_elem[col] = normalize_rows(counts_elem, axis=2)\n",
    "\n",
    "    def update_hidden_CPT(self):\n",
    "        \"\"\"\n",
    "        Update P(H | Region, FieldDate)\n",
    "        Accumulate weighted counts per condition using responsibilities calculated above\n",
    "        \"\"\"\n",
    "        # identical to part of m_step\n",
    "        counts_H_RY = np.zeros((self.K_region, self.K_year, self.H))\n",
    "        idx_region = self.df[self.region_col].map(self.maps[self.region_col]).values\n",
    "        idx_year = self.df[self.year_col].map(self.maps[self.year_col]).values\n",
    "        for i in range(self.N):\n",
    "            r = idx_region[i]; y = idx_year[i]\n",
    "            counts_H_RY[r, y, :] += self.gamma[i, :]\n",
    "        self.P_H_given_RY = normalize_rows(counts_H_RY, axis=2)\n",
    "\n",
    "    def update_pollution_CPT(self):\n",
    "        \"\"\"\n",
    "        Update P(Pollution | H)\n",
    "        Pollution is observed, responsibilities provides the weighting.\n",
    "        \"\"\"\n",
    "        counts_Pe_H = np.zeros((self.H, self.K_poll))\n",
    "        idxPoll = self.df[self.pollution_col].map(self.maps[self.pollution_col]).values\n",
    "        for i in range(self.N):\n",
    "            p = idxPoll[i]\n",
    "            counts_Pe_H[:, p] += self.gamma[i, :]\n",
    "        self.P_Pe_given_H = normalize_rows(counts_Pe_H, axis=1)\n",
    "\n",
    "    def update_species_CPT(self):\n",
    "        \"\"\"\n",
    "        Update: P(Species | H, Pollution)\n",
    "        Species is observed, conditioned on hidden state and pollution bucket(S)\n",
    "        \"\"\"\n",
    "        counts_Sp_HPe = np.zeros((self.H, self.K_poll, self.K_species))\n",
    "        idx_species = self.df[self.species_col].map(self.maps[self.species_col]).values\n",
    "        idx_poll = self.df[self.pollution_col].map(self.maps[self.pollution_col]).values\n",
    "        for i in range(self.N):\n",
    "            p = idx_poll[i]\n",
    "            s = idx_species[i]\n",
    "            counts_Sp_HPe[:, p, s] += self.gamma[i, :]\n",
    "        self.P_Sp_given_HPe = normalize_rows(counts_Sp_HPe, axis=2)\n",
    "\n",
    "    def update_element_bucket_CPTs(self):\n",
    "        \"\"\"\n",
    "        For each tissue element bucket node:\n",
    "            Update: P(ElementBucket | Species, Pollution)\n",
    "        Summed over hidden responsibilities since element buckets have no H parent\n",
    "        \"\"\"\n",
    "        idx_species = self.df[self.species_col].map(self.maps[self.species_col]).values\n",
    "        idx_poll = self.df[self.pollution_col].map(self.maps[self.pollution_col]).values\n",
    "        N = self.N\n",
    "        Kp = self.K_poll\n",
    "        for col in self.elements:\n",
    "            counts_elem = np.zeros((Kp, self.K_species, self.K_elem[col]))\n",
    "            for i in range(N):\n",
    "                p = idx_poll[i]\n",
    "                s = idx_species[i]\n",
    "                val = self.maps[col][self.df[col].iloc[i]]\n",
    "                counts_elem[p, s, val] += 1.0\n",
    "            self.P_elem[col] = normalize_rows(counts_elem, axis=2)\n",
    "\n",
    "    def log_likelihood(self):\n",
    "        \"\"\"\n",
    "        Observed-data log-likelihood:\n",
    "        sum_i log P(observed_i) = sum_i log sum_h P(H|r,y) P(Pe|H) P(Sp|H) prod_k P(elem_k | Pe, Sp)\n",
    "        If Pe is missing for some rows (not expected in training) we marginalize over Pe as well.\n",
    "        \"\"\"\n",
    "        idx_region = self.df[self.region_col].map(self.maps[self.region_col]).values\n",
    "        idx_year = self.df[self.year_col].map(self.maps[self.year_col]).values\n",
    "        idx_species = self.df[self.species_col].map(self.maps[self.species_col]).values\n",
    "        idx_poll = self.df[self.pollution_col].map(self.maps[self.pollution_col]).values\n",
    "\n",
    "        total = 0.0\n",
    "        for i in range(self.N):\n",
    "            r = idx_region[i]; y = idx_year[i]; s = idx_species[i]\n",
    "            # compute log evidence per h (if Pe observed)\n",
    "            p_obs = idx_poll[i]\n",
    "            log_ph = np.log(self.P_H_given_RY[r, y, :] + EPS)  # (H,)\n",
    "            # sum element log-likelihood (depends on pe)\n",
    "            if pd.notna(self.df[self.pollution_col].iloc[i]):\n",
    "                # Pe observed\n",
    "                log_ppe = np.log(self.P_Pe_given_H[:, p_obs] + EPS)\n",
    "                log_psp = np.log(self.P_Sp_given_HPe[:, p_obs, s] + EPS)\n",
    "                # element contributions (same for all h)\n",
    "                log_elems = 0.0\n",
    "                for col in self.elements:\n",
    "                    val = self.maps[col][self.df[col].iloc[i]]\n",
    "                    log_elems += np.log(self.P_elem[col][p_obs, s, val] + EPS)\n",
    "                log_u = log_ph +log_ppe + log_psp + log_elems\n",
    "                total += float(logsumexp(log_u, axis=0))\n",
    "            else:\n",
    "                # Pe missing\n",
    "                pass\n",
    "                # compute log_ppe[h,pe] + log_elem(pe) foreach var\n",
    "                Kp = self.K_poll\n",
    "                log_contrib = np.zeros((Kp, self.H))\n",
    "                elem_vals = {col: self.maps[col][self.df[col].iloc[i]] for col in self.elements}\n",
    "                for pe in range(Kp):\n",
    "                    logppe = np.log(self.P_Pe_given_H[:, pe] + EPS)\n",
    "                    log_elems = 0.0\n",
    "                    for col in self.elements:\n",
    "                        log_elems += np.log(self.P_elem[col][pe, s, elem_vals[col]] + EPS)\n",
    "                    log_contrib[pe, :] = logppe + log_elems\n",
    "                # for each h: log_p_h = log_ph + logsumexp_over_pe(log_contrib[:,h])\n",
    "                log_sum_pe_per_h = logsumexp(log_contrib, axis=0)\n",
    "                log_u_h = log_ph + log_sum_pe_per_h\n",
    "                total += float(logsumexp(log_u_h, axis=0))\n",
    "        return total\n",
    "\n",
    "    def run(self, max_iters=100, tol=1e-6, verbose=True):\n",
    "        \"\"\"\n",
    "        Full EM loop:\n",
    "        initialize_random_CPTs()\n",
    "        e_step()\n",
    "        m_step()\n",
    "        compute log likelihood and check tolerance\n",
    "        Repeat.\n",
    "        Returns learned CPT parameters.\n",
    "        \"\"\"\n",
    "        lls= []\n",
    "        prev_ll = -np.inf\n",
    "        for it in range(1, max_iters+1):\n",
    "            self.e_step()\n",
    "            self.m_step()\n",
    "            ll = self.log_likelihood()\n",
    "            lls.append(ll)\n",
    "            if verbose:\n",
    "                print(f\"Iter {it:3d}  ll = {ll:.6f}\")\n",
    "            if np.isfinite(prev_ll) and abs(ll - prev_ll) < tol:\n",
    "                if verbose:\n",
    "                    print(\"Converged (tol).\")\n",
    "                break\n",
    "            prev_ll = ll\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(lls, marker='o')\n",
    "        plt.title(\"EM Training Log-Likelihood\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Log-Likelihood\")\n",
    "        plt.grid(True)\n",
    "        os.makedirs(\"plots\", exist_ok=True)\n",
    "        save_path = \"plots/log_likelihood_curve.png\"\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "        return {\n",
    "            'P_H_given_RY': self.P_H_given_RY,\n",
    "            'P_Pe_given_H': self.P_Pe_given_H,\n",
    "            'P_Sp_given_H': self.P_Sp_given_HPe,\n",
    "            'P_elem': self.P_elem\n",
    "        }\n",
    "\n",
    "    def predict_pollution_distribution(self, row):\n",
    "        \"\"\"\n",
    "        Compute P(Pollution | observed row)\n",
    "        Uses:\n",
    "            P(Pe | row) = sum_h P(Pe | H=h) * P(H=h | row)\n",
    "        \"\"\"\n",
    "        # compute posterior P(H | row)\n",
    "        p_h = self.compute_posterior_hidden_probs(row)\n",
    "        p_pe = (p_h[:, None] * self.P_Pe_given_H).sum(axis=0)\n",
    "        p_pe = p_pe / (p_pe.sum() + EPS)\n",
    "        return p_pe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b3ceca82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   1  ll = -45155.837438\n",
      "Iter   2  ll = -45114.606336\n",
      "Iter   3  ll = -45101.784938\n",
      "Iter   4  ll = -45095.777810\n",
      "Iter   5  ll = -45091.430278\n",
      "Iter   6  ll = -45087.350140\n",
      "Iter   7  ll = -45083.176200\n",
      "Iter   8  ll = -45078.846012\n",
      "Iter   9  ll = -45074.370626\n",
      "Iter  10  ll = -45069.762442\n",
      "Iter  11  ll = -45065.027561\n",
      "Iter  12  ll = -45060.182063\n",
      "Iter  13  ll = -45055.265722\n",
      "Iter  14  ll = -45050.344138\n",
      "Iter  15  ll = -45045.500704\n",
      "Iter  16  ll = -45040.822670\n",
      "Iter  17  ll = -45036.386428\n",
      "Iter  18  ll = -45032.246837\n",
      "Iter  19  ll = -45028.433153\n",
      "Iter  20  ll = -45024.951045\n",
      "Iter  21  ll = -45021.788143\n",
      "Iter  22  ll = -45018.920453\n",
      "Iter  23  ll = -45016.317915\n",
      "Iter  24  ll = -45013.948448\n",
      "Iter  25  ll = -45011.780539\n",
      "Iter  26  ll = -45009.784699\n",
      "Iter  27  ll = -45007.934152\n",
      "Iter  28  ll = -45006.205059\n",
      "Iter  29  ll = -45004.576470\n",
      "Iter  30  ll = -45003.030147\n",
      "Iter  31  ll = -45001.550330\n",
      "Iter  32  ll = -45000.123491\n",
      "Iter  33  ll = -44998.738087\n",
      "Iter  34  ll = -44997.384346\n",
      "Iter  35  ll = -44996.054056\n",
      "Iter  36  ll = -44994.740389\n",
      "Iter  37  ll = -44993.437726\n",
      "Iter  38  ll = -44992.141515\n",
      "Iter  39  ll = -44990.848127\n",
      "Iter  40  ll = -44989.554734\n",
      "Iter  41  ll = -44988.259194\n",
      "Iter  42  ll = -44986.959947\n",
      "Iter  43  ll = -44985.655924\n",
      "Iter  44  ll = -44984.346465\n",
      "Iter  45  ll = -44983.031250\n",
      "Iter  46  ll = -44981.710238\n",
      "Iter  47  ll = -44980.383619\n",
      "Iter  48  ll = -44979.051777\n",
      "Iter  49  ll = -44977.715257\n",
      "Iter  50  ll = -44976.374740\n",
      "Iter  51  ll = -44975.031028\n",
      "Iter  52  ll = -44973.685020\n",
      "Iter  53  ll = -44972.337704\n",
      "Iter  54  ll = -44970.990133\n",
      "Iter  55  ll = -44969.643415\n",
      "Iter  56  ll = -44968.298689\n",
      "Iter  57  ll = -44966.957105\n",
      "Iter  58  ll = -44965.619796\n",
      "Iter  59  ll = -44964.287861\n",
      "Iter  60  ll = -44962.962335\n",
      "Iter  61  ll = -44961.644168\n",
      "Iter  62  ll = -44960.334204\n",
      "Iter  63  ll = -44959.033169\n",
      "Iter  64  ll = -44957.741652\n",
      "Iter  65  ll = -44956.460105\n",
      "Iter  66  ll = -44955.188840\n",
      "Iter  67  ll = -44953.928030\n",
      "Iter  68  ll = -44952.677719\n",
      "Iter  69  ll = -44951.437831\n",
      "Iter  70  ll = -44950.208187\n",
      "Iter  71  ll = -44948.988515\n",
      "Iter  72  ll = -44947.778468\n",
      "Iter  73  ll = -44946.577641\n",
      "Iter  74  ll = -44945.385582\n",
      "Iter  75  ll = -44944.201811\n",
      "Iter  76  ll = -44943.025828\n",
      "Iter  77  ll = -44941.857128\n",
      "Iter  78  ll = -44940.695212\n",
      "Iter  79  ll = -44939.539592\n",
      "Iter  80  ll = -44938.389805\n",
      "Iter  81  ll = -44937.245422\n",
      "Iter  82  ll = -44936.106052\n",
      "Iter  83  ll = -44934.971357\n",
      "Iter  84  ll = -44933.841060\n",
      "Iter  85  ll = -44932.714952\n",
      "Iter  86  ll = -44931.592912\n",
      "Iter  87  ll = -44930.474911\n",
      "Iter  88  ll = -44929.361033\n",
      "Iter  89  ll = -44928.251484\n",
      "Iter  90  ll = -44927.146609\n",
      "Iter  91  ll = -44926.046907\n",
      "Iter  92  ll = -44924.953039\n",
      "Iter  93  ll = -44923.865845\n",
      "Iter  94  ll = -44922.786348\n",
      "Iter  95  ll = -44921.715761\n",
      "Iter  96  ll = -44920.655485\n",
      "Iter  97  ll = -44919.607105\n",
      "Iter  98  ll = -44918.572379\n",
      "Iter  99  ll = -44917.553220\n",
      "Iter 100  ll = -44916.551673\n",
      "Iter 101  ll = -44915.569886\n",
      "Iter 102  ll = -44914.610070\n",
      "Iter 103  ll = -44913.674466\n",
      "Iter 104  ll = -44912.765296\n",
      "Iter 105  ll = -44911.884717\n",
      "Iter 106  ll = -44911.034778\n",
      "Iter 107  ll = -44910.217371\n",
      "Iter 108  ll = -44909.434190\n",
      "Iter 109  ll = -44908.686693\n",
      "Iter 110  ll = -44907.976072\n",
      "Iter 111  ll = -44907.303227\n",
      "Iter 112  ll = -44906.668752\n",
      "Iter 113  ll = -44906.072925\n",
      "Iter 114  ll = -44905.515712\n",
      "Iter 115  ll = -44904.996777\n",
      "Iter 116  ll = -44904.515495\n",
      "Iter 117  ll = -44904.070981\n",
      "Iter 118  ll = -44903.662109\n",
      "Iter 119  ll = -44903.287553\n",
      "Iter 120  ll = -44902.945813\n",
      "Iter 121  ll = -44902.635253\n",
      "Iter 122  ll = -44902.354133\n",
      "Iter 123  ll = -44902.100642\n",
      "Iter 124  ll = -44901.872928\n",
      "Iter 125  ll = -44901.669129\n",
      "Iter 126  ll = -44901.487393\n",
      "Iter 127  ll = -44901.325906\n",
      "Iter 128  ll = -44901.182906\n",
      "Iter 129  ll = -44901.056699\n",
      "Iter 130  ll = -44900.945675\n",
      "Iter 131  ll = -44900.848311\n",
      "Iter 132  ll = -44900.763185\n",
      "Iter 133  ll = -44900.688972\n",
      "Iter 134  ll = -44900.624453\n",
      "Iter 135  ll = -44900.568508\n",
      "Iter 136  ll = -44900.520118\n",
      "Iter 137  ll = -44900.478362\n",
      "Iter 138  ll = -44900.442409\n",
      "Iter 139  ll = -44900.411517\n",
      "Iter 140  ll = -44900.385025\n",
      "Iter 141  ll = -44900.362344\n",
      "Iter 142  ll = -44900.342959\n",
      "Iter 143  ll = -44900.326413\n",
      "Iter 144  ll = -44900.312309\n",
      "Iter 145  ll = -44900.300299\n",
      "Iter 146  ll = -44900.290083\n",
      "Iter 147  ll = -44900.281397\n",
      "Iter 148  ll = -44900.274016\n",
      "Iter 149  ll = -44900.267747\n",
      "Iter 150  ll = -44900.262421\n",
      "Iter 151  ll = -44900.257895\n",
      "Iter 152  ll = -44900.254047\n",
      "Iter 153  ll = -44900.250773\n",
      "Iter 154  ll = -44900.247982\n",
      "Iter 155  ll = -44900.245601\n",
      "Iter 156  ll = -44900.243563\n",
      "Iter 157  ll = -44900.241815\n",
      "Iter 158  ll = -44900.240310\n",
      "Iter 159  ll = -44900.239010\n",
      "Iter 160  ll = -44900.237881\n",
      "Iter 161  ll = -44900.236895\n",
      "Iter 162  ll = -44900.236030\n",
      "Iter 163  ll = -44900.235265\n",
      "Iter 164  ll = -44900.234583\n",
      "Iter 165  ll = -44900.233971\n",
      "Iter 166  ll = -44900.233418\n",
      "Iter 167  ll = -44900.232912\n",
      "Iter 168  ll = -44900.232446\n",
      "Iter 169  ll = -44900.232012\n",
      "Iter 170  ll = -44900.231606\n",
      "Iter 171  ll = -44900.231221\n",
      "Iter 172  ll = -44900.230854\n",
      "Iter 173  ll = -44900.230502\n",
      "Iter 174  ll = -44900.230161\n",
      "Iter 175  ll = -44900.229829\n",
      "Iter 176  ll = -44900.229505\n",
      "Iter 177  ll = -44900.229185\n",
      "Iter 178  ll = -44900.228870\n",
      "Iter 179  ll = -44900.228557\n",
      "Iter 180  ll = -44900.228246\n",
      "Iter 181  ll = -44900.227935\n",
      "Iter 182  ll = -44900.227625\n",
      "Iter 183  ll = -44900.227314\n",
      "Iter 184  ll = -44900.227003\n",
      "Iter 185  ll = -44900.226690\n",
      "Iter 186  ll = -44900.226375\n",
      "Iter 187  ll = -44900.226058\n",
      "Iter 188  ll = -44900.225739\n",
      "Iter 189  ll = -44900.225417\n",
      "Iter 190  ll = -44900.225093\n",
      "Iter 191  ll = -44900.224766\n",
      "Iter 192  ll = -44900.224435\n",
      "Iter 193  ll = -44900.224102\n",
      "Iter 194  ll = -44900.223765\n",
      "Iter 195  ll = -44900.223426\n",
      "Iter 196  ll = -44900.223083\n",
      "Iter 197  ll = -44900.222736\n",
      "Iter 198  ll = -44900.222386\n",
      "Iter 199  ll = -44900.222033\n",
      "Iter 200  ll = -44900.221676\n",
      "Pollution bucket prediction accuracy: 0.724\n"
     ]
    }
   ],
   "source": [
    "cols_we_keep = [\n",
    "    'Nitrogen (% dw)_binned',\n",
    "    'Sulfur (% dw)_binned',\n",
    "    'Phosphorous (ppm dw)_binned',\n",
    "    'Lead (ppm dw)_binned',\n",
    "    'Copper (ppm dw)_binned',\n",
    "    'Chromium (ppm dw)_binned',\n",
    "    'Year of tissue collection_binned',\n",
    "    'Air pollution score_binned',\n",
    "    'Region_binned',\n",
    "    'Code for scientific name and authority in lookup table_binned'\n",
    "]\n",
    "df_clean = df[cols_we_keep].dropna().reset_index(drop=True)\n",
    "\n",
    "train_df, test_df = train_test_split(df_clean, test_size=0.2, random_state=42)\n",
    "# train em\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "model = BayesianNetworkEM(train_df,hidden_states=2, seed=0) \n",
    "\n",
    "model.run(max_iters=200, tol=1e-6)\n",
    "\n",
    "# eval on test set\n",
    "true_test_labels = test_df['Air pollution score_binned'].values\n",
    "poll_mapping = model.maps[model.pollution_col]\n",
    "predicted_labels = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    probs = model.predict_pollution_distribution(row)\n",
    "    # Pick most likely bucket\n",
    "    pred_idx = np.argmax(probs)\n",
    "    pred_label = model.rev[model.pollution_col][pred_idx]\n",
    "    predicted_labels.append(pred_label)\n",
    "\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "# accuracy score\n",
    "accuracy = np.mean(predicted_labels == true_test_labels)\n",
    "print(f\"Pollution bucket prediction accuracy: {accuracy:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
