{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6044d56f",
   "metadata": {},
   "source": [
    "# Lichen Likelihood Project: Expectation Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb1520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cd5f9173",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_df = pd.read_pickle('element_analysis.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2eeb8158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Nitrogen (% dw)_binned', 'Sulfur (% dw)_binned',\n",
       "       'Phosphorous (ppm dw)_binned', 'Lead (ppm dw)_binned',\n",
       "       'Copper (ppm dw)_binned', 'Chromium (ppm dw)_binned',\n",
       "       'Year of tissue collection_binned', 'Air pollution score_binned',\n",
       "       'Region_binned',\n",
       "       'Code for scientific name and authority in lookup table_binned'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print columns ending in \"binned\"\n",
    "unpickled_df.columns[unpickled_df.columns.str.endswith('binned')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631ef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "df = unpickled_df.copy()\n",
    "def idx_map_from_series(s):\n",
    "    \"\"\"Return dict mapping original category\"\"\"\n",
    "    cats = list(pd.Categorical(s).categories)\n",
    "    mapping = {c:i for i,c in enumerate(cats)}\n",
    "    return mapping, cats\n",
    "\n",
    "def one_hot_index_from_value(mapping, val):\n",
    "    return mapping[val]\n",
    "\n",
    "def normalize_rows(mat, axis=-1, alpha=1e-2):\n",
    "    \"\"\"\n",
    "    Normalize\n",
    "    \"\"\"\n",
    "    arr = mat.astype(float).copy()\n",
    "    arr += float(alpha)\n",
    "    s = arr.sum(axis=axis, keepdims=True)\n",
    "    s[s == 0] = 1.0\n",
    "    return arr / s\n",
    "\n",
    "def logsumexp(a, axis=None): # self explanatory\n",
    "    a_max = np.max(a, axis=axis, keepdims=True)\n",
    "    res = a_max + np.log(np.sum(np.exp(a - a_max), axis=axis, keepdims=True))\n",
    "    if axis is not None:\n",
    "        return np.squeeze(res, axis=axis)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-10\n",
    "class BayesianNetworkEM:\n",
    "    def __init__(self, df, hidden_states=2, seed=0):\n",
    "        \"\"\"\n",
    "        Initializes CPT tables with random values consistent with the BN structure.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.N = len(self.df)\n",
    "        self.H = hidden_states\n",
    "        # element node column names\n",
    "        self.elements = [\n",
    "            'Nitrogen (% dw)_binned',\n",
    "            'Sulfur (% dw)_binned',\n",
    "            'Phosphorous (ppm dw)_binned',\n",
    "            'Lead (ppm dw)_binned',\n",
    "            'Copper (ppm dw)_binned',\n",
    "            'Chromium (ppm dw)_binned'\n",
    "        ]\n",
    "        self.year_col = 'Year of tissue collection_binned'\n",
    "        self.pollution_col = 'Air pollution score_binned'\n",
    "        self.region_col = 'Region_binned'\n",
    "        self.species_col = 'Code for scientific name and authority in lookup table_binned'\n",
    "        self.maps = {}\n",
    "        self.rev = {}\n",
    "        cols_for_map = self.elements + [self.year_col, self.pollution_col, self.region_col, self.species_col]\n",
    "        for col in cols_for_map:\n",
    "            mapping, cats = idx_map_from_series(self.df[col])\n",
    "            self.maps[col] = mapping\n",
    "            self.rev[col] = cats\n",
    "\n",
    "        self.K_region = len(self.rev[self.region_col])\n",
    "        self.K_year = len(self.rev[self.year_col])\n",
    "        self.K_poll = len(self.rev[self.pollution_col])\n",
    "        self.K_species = len(self.rev[self.species_col])\n",
    "        self.K_elem = {col: len(self.rev[col]) for col in self.elements}\n",
    "        self.initialize_random_CPTs()\n",
    "        self.gamma = np.zeros((self.N, self.H))\n",
    "\n",
    "    def initialize_random_CPTs(self):\n",
    "        \"\"\"\n",
    "        Randomly initialize all CPTs:\n",
    "        Ensure each CPT is normalized over its conditional domain.\n",
    "        \"\"\"\n",
    "        self.P_H_given_RY = normalize_rows(np.random.rand(self.K_region, self.K_year, self.H), axis=2)\n",
    "\n",
    "        self.P_Pe_given_H = normalize_rows(np.random.rand(self.H, self.K_poll), axis=1)\n",
    "\n",
    "        # model P(Sp | H, Pe)\n",
    "        self.P_Sp_given_HPe = normalize_rows(\n",
    "            np.random.rand(self.H, self.K_poll, self.K_species), axis=2\n",
    "        )\n",
    "\n",
    "        # elements depend on (Pe, Sp)\n",
    "        self.P_elem = {}\n",
    "        for col in self.elements:\n",
    "            arr = np.random.rand(self.K_poll, self.K_species, self.K_elem[col])\n",
    "            self.P_elem[col] = normalize_rows(arr, axis=2)\n",
    "\n",
    "    def e_step(self):\n",
    "        \"\"\"\n",
    "        Performs the E-step\n",
    "            For each sample n:\n",
    "                Compute posterior responsibilities\n",
    "            Store in self.gamma with shape [num_samples, num_hidden_states]\n",
    "        \"\"\"\n",
    "        idx_region = self.df[self.region_col].map(self.maps[self.region_col]).values\n",
    "        idx_year = self.df[self.year_col].map(self.maps[self.year_col]).values\n",
    "        idx_species = self.df[self.species_col].map(self.maps[self.species_col]).values\n",
    "        idx_poll = self.df[self.pollution_col].map(self.maps[self.pollution_col]).values\n",
    "\n",
    "        N = self.N\n",
    "        H = self.H\n",
    "        Kp = self.K_poll\n",
    "\n",
    "        for i in range(N):\n",
    "            r = idx_region[i]\n",
    "            y = idx_year[i]\n",
    "            sp = idx_species[i]\n",
    "            p_val = idx_poll[i]\n",
    "            # log P(H | r,y)\n",
    "            log_ph = np.log(self.P_H_given_RY[r, y, :] + EPS)\n",
    "            # log P(Pe | H)\n",
    "            log_ppe = np.log(self.P_Pe_given_H[:, p_val] + EPS)\n",
    "            # log P(Sp | H, Pe)\n",
    "            log_psp = np.log(self.P_Sp_given_HPe[:, p_val, sp] + EPS)\n",
    "            # elements are d seperated from H when Pe and Sp are observed\n",
    "            log_unnorm = log_ph + log_ppe + log_psp\n",
    "            log_denom = logsumexp(log_unnorm, axis=0)\n",
    "            self.gamma[i, :] = np.exp(log_unnorm - log_denom)\n",
    "\n",
    "        return self.gamma\n",
    "\n",
    "    def compute_posterior_hidden_probs(self, sample_row):\n",
    "        \"\"\"\n",
    "        Compute P(H=h | observed sample features) for a single sample\n",
    "        Returns a vector normalized over hidden states\n",
    "        \"\"\"\n",
    "        r = self.maps[self.region_col][sample_row[self.region_col]]\n",
    "        y = self.maps[self.year_col][sample_row[self.year_col]]\n",
    "        s = self.maps[self.species_col][sample_row[self.species_col]]\n",
    "\n",
    "        log_ph = np.log(self.P_H_given_RY[r, y, :] + EPS)\n",
    "        if self.pollution_col in sample_row.index and pd.notna(sample_row[self.pollution_col]):\n",
    "            p_obs = self.maps[self.pollution_col][sample_row[self.pollution_col]]\n",
    "            log_ppe = np.log(self.P_Pe_given_H[:, p_obs] + EPS)\n",
    "            # include Sp | Pe\n",
    "            log_psp_obs = np.log(self.P_Sp_given_HPe[:, p_obs, s] + EPS)\n",
    "            log_unnorm = log_ph + log_ppe + log_psp_obs\n",
    "            log_norm = logsumexp(log_unnorm, axis=0)\n",
    "            return np.exp(log_unnorm - log_norm)\n",
    "\n",
    "        # log_contrib_pe[h] = log P(Pe=pe | H=h) + log P(Sp=s | H,Pe=pe) + sum_k log P(elem_k | pe, s) forall (updates pe)\n",
    "        Kp = self.K_poll\n",
    "        log_contrib = np.zeros((Kp, self.H)) \n",
    "        elem_vals = {}\n",
    "        for col in self.elements:\n",
    "            elem_vals[col] = self.maps[col][sample_row[col]]\n",
    "        for pe in range(Kp):\n",
    "            # log P(Pe=pe | H)\n",
    "            logppe = np.log(self.P_Pe_given_H[:, pe] + EPS)\n",
    "            # log P(Sp = s | H, Pe=pe)\n",
    "            log_psp_pe = np.log(self.P_Sp_given_HPe[:, pe, s] + EPS)\n",
    "            # sum over log P(elem | pe, s)\n",
    "            log_elems = 0.0\n",
    "            for col in self.elements:\n",
    "                log_elems += np.log(self.P_elem[col][pe, s, elem_vals[col]] + EPS)\n",
    "            # log contribution per h\n",
    "            log_contrib[pe, :] = logppe + log_psp_pe + log_elems\n",
    "\n",
    "        # log_p_h = log_ph + logsum_pe [ logppe + log_psp_pe + log_elems ] (update for each hidden)\n",
    "        log_sum_pe_per_h = logsumexp(log_contrib, axis=0)\n",
    "        log_p_h = log_ph + log_sum_pe_per_h\n",
    "        log_norm = logsumexp(log_p_h, axis=0)\n",
    "        return np.exp(log_p_h - log_norm)\n",
    "\n",
    "    def m_step(self):\n",
    "        \"\"\"\n",
    "        Performs the M-step:\n",
    "            Update CPTs:\n",
    "                - P(H | Region, FieldDate)\n",
    "                - P(Pollution | H)\n",
    "                - P(Species | H, Pollution)\n",
    "                - P(Element_i_bucket | Species, Pollution) for each element node\n",
    "        Uses expected counts\n",
    "        \"\"\"\n",
    "        # precompute indices\n",
    "        idx_region = self.df[self.region_col].map(self.maps[self.region_col]).values\n",
    "        idx_year = self.df[self.year_col].map(self.maps[self.year_col]).values\n",
    "        idx_species = self.df[self.species_col].map(self.maps[self.species_col]).values\n",
    "        idx_poll = self.df[self.pollution_col].map(self.maps[self.pollution_col]).values\n",
    "        N = self.N\n",
    "        H = self.H\n",
    "        Kp = self.K_poll\n",
    "\n",
    "        #P(H | Region, Year) counts\n",
    "        counts_H_RY = np.zeros((self.K_region, self.K_year, H))\n",
    "        for i in range(N):\n",
    "            r = idx_region[i]; y = idx_year[i]\n",
    "            counts_H_RY[r, y, :] += self.gamma[i, :] \n",
    "        self.P_H_given_RY = normalize_rows(counts_H_RY, axis=2)\n",
    "        # P(Pe | H)\n",
    "        counts_Pe_H = np.zeros((H, Kp))\n",
    "        for i in range(N):\n",
    "            p = idx_poll[i]\n",
    "            counts_Pe_H[:, p] += self.gamma[i, :]\n",
    "        # counts_Pe_H \n",
    "        self.P_Pe_given_H = normalize_rows(counts_Pe_H, axis=1)\n",
    "        # P(Sp | H, Pe)\n",
    "        counts_Sp_HPe = np.zeros((H, Kp, self.K_species))\n",
    "        for i in range(N):\n",
    "            p = idx_poll[i]\n",
    "            s = idx_species[i]\n",
    "            counts_Sp_HPe[:, p, s] += self.gamma[i, :]\n",
    "        # normalize over species\n",
    "        self.P_Sp_given_HPe = normalize_rows(counts_Sp_HPe, axis=2)\n",
    "\n",
    "        # 4) P(elem | Pe, Sp) doesnt need gamma bc dsep\n",
    "        for col in self.elements:\n",
    "            Kval = self.K_elem[col]\n",
    "            counts_elem = np.zeros((Kp, self.K_species, Kval))\n",
    "            for i in range(N):\n",
    "                p = idx_poll[i]\n",
    "                s = idx_species[i]\n",
    "                val = self.maps[col][self.df[col].iloc[i]]\n",
    "                counts_elem[p, s, val] += 1.0\n",
    "            self.P_elem[col] = normalize_rows(counts_elem, axis=2)\n",
    "\n",
    "    def update_hidden_CPT(self):\n",
    "        \"\"\"\n",
    "        Update P(H | Region, FieldDate)\n",
    "        Accumulate weighted counts per condition using responsibilities calculated above\n",
    "        \"\"\"\n",
    "        # identical to part of m_step\n",
    "        counts_H_RY = np.zeros((self.K_region, self.K_year, self.H))\n",
    "        idx_region = self.df[self.region_col].map(self.maps[self.region_col]).values\n",
    "        idx_year = self.df[self.year_col].map(self.maps[self.year_col]).values\n",
    "        for i in range(self.N):\n",
    "            r = idx_region[i]; y = idx_year[i]\n",
    "            counts_H_RY[r, y, :] += self.gamma[i, :]\n",
    "        self.P_H_given_RY = normalize_rows(counts_H_RY, axis=2)\n",
    "\n",
    "    def update_pollution_CPT(self):\n",
    "        \"\"\"\n",
    "        Update P(Pollution | H)\n",
    "        Pollution is observed, responsibilities provides the weighting.\n",
    "        \"\"\"\n",
    "        counts_Pe_H = np.zeros((self.H, self.K_poll))\n",
    "        idxPoll = self.df[self.pollution_col].map(self.maps[self.pollution_col]).values\n",
    "        for i in range(self.N):\n",
    "            p = idxPoll[i]\n",
    "            counts_Pe_H[:, p] += self.gamma[i, :]\n",
    "        self.P_Pe_given_H = normalize_rows(counts_Pe_H, axis=1)\n",
    "\n",
    "    def update_species_CPT(self):\n",
    "        \"\"\"\n",
    "        Update: P(Species | H, Pollution)\n",
    "        Species is observed, conditioned on hidden state and pollution bucket(S)\n",
    "        \"\"\"\n",
    "        counts_Sp_HPe = np.zeros((self.H, self.K_poll, self.K_species))\n",
    "        idx_species = self.df[self.species_col].map(self.maps[self.species_col]).values\n",
    "        idx_poll = self.df[self.pollution_col].map(self.maps[self.pollution_col]).values\n",
    "        for i in range(self.N):\n",
    "            p = idx_poll[i]\n",
    "            s = idx_species[i]\n",
    "            counts_Sp_HPe[:, p, s] += self.gamma[i, :]\n",
    "        self.P_Sp_given_HPe = normalize_rows(counts_Sp_HPe, axis=2)\n",
    "\n",
    "    def update_element_bucket_CPTs(self):\n",
    "        \"\"\"\n",
    "        For each tissue element bucket node:\n",
    "            Update: P(ElementBucket | Species, Pollution)\n",
    "        Summed over hidden responsibilities since element buckets have no H parent\n",
    "        \"\"\"\n",
    "        idx_species = self.df[self.species_col].map(self.maps[self.species_col]).values\n",
    "        idx_poll = self.df[self.pollution_col].map(self.maps[self.pollution_col]).values\n",
    "        N = self.N\n",
    "        Kp = self.K_poll\n",
    "        for col in self.elements:\n",
    "            counts_elem = np.zeros((Kp, self.K_species, self.K_elem[col]))\n",
    "            for i in range(N):\n",
    "                p = idx_poll[i]\n",
    "                s = idx_species[i]\n",
    "                val = self.maps[col][self.df[col].iloc[i]]\n",
    "                counts_elem[p, s, val] += 1.0\n",
    "            self.P_elem[col] = normalize_rows(counts_elem, axis=2)\n",
    "\n",
    "    def log_likelihood(self):\n",
    "        \"\"\"\n",
    "        Observed-data log-likelihood:\n",
    "        sum_i log P(observed_i) = sum_i log sum_h P(H|r,y) P(Pe|H) P(Sp|H) prod_k P(elem_k | Pe, Sp)\n",
    "        If Pe is missing for some rows (not expected in training) we marginalize over Pe as well.\n",
    "        \"\"\"\n",
    "        idx_region = self.df[self.region_col].map(self.maps[self.region_col]).values\n",
    "        idx_year = self.df[self.year_col].map(self.maps[self.year_col]).values\n",
    "        idx_species = self.df[self.species_col].map(self.maps[self.species_col]).values\n",
    "        idx_poll = self.df[self.pollution_col].map(self.maps[self.pollution_col]).values\n",
    "\n",
    "        total = 0.0\n",
    "        for i in range(self.N):\n",
    "            r = idx_region[i]; y = idx_year[i]; s = idx_species[i]\n",
    "            # compute log evidence per h (if Pe observed)\n",
    "            p_obs = idx_poll[i]\n",
    "            log_ph = np.log(self.P_H_given_RY[r, y, :] + EPS)  # (H,)\n",
    "            # sum element log-likelihood (depends on pe)\n",
    "            if pd.notna(self.df[self.pollution_col].iloc[i]):\n",
    "                # Pe observed\n",
    "                log_ppe = np.log(self.P_Pe_given_H[:, p_obs] + EPS)\n",
    "                log_psp = np.log(self.P_Sp_given_HPe[:, p_obs, s] + EPS)\n",
    "                # element contributions (same for all h)\n",
    "                log_elems = 0.0\n",
    "                for col in self.elements:\n",
    "                    val = self.maps[col][self.df[col].iloc[i]]\n",
    "                    log_elems += np.log(self.P_elem[col][p_obs, s, val] + EPS)\n",
    "                log_u = log_ph +log_ppe + log_psp + log_elems\n",
    "                total += float(logsumexp(log_u, axis=0))\n",
    "            else:\n",
    "                # Pe missing\n",
    "                pass\n",
    "                # compute log_ppe[h,pe] + log_elem(pe) foreach var\n",
    "                Kp = self.K_poll\n",
    "                log_contrib = np.zeros((Kp, self.H))\n",
    "                elem_vals = {col: self.maps[col][self.df[col].iloc[i]] for col in self.elements}\n",
    "                for pe in range(Kp):\n",
    "                    logppe = np.log(self.P_Pe_given_H[:, pe] + EPS)\n",
    "                    log_elems = 0.0\n",
    "                    for col in self.elements:\n",
    "                        log_elems += np.log(self.P_elem[col][pe, s, elem_vals[col]] + EPS)\n",
    "                    log_contrib[pe, :] = logppe + log_elems\n",
    "                # for each h: log_p_h = log_ph + logsumexp_over_pe(log_contrib[:,h])\n",
    "                log_sum_pe_per_h = logsumexp(log_contrib, axis=0)\n",
    "                log_u_h = log_ph + log_sum_pe_per_h\n",
    "                total += float(logsumexp(log_u_h, axis=0))\n",
    "        return total\n",
    "\n",
    "    def run(self, max_iters=500, tol=1e-6, verbose=True):\n",
    "        \"\"\"\n",
    "        Full EM loop:\n",
    "        initialize_random_CPTs()\n",
    "        e_step()\n",
    "        m_step()\n",
    "        compute log likelihood and check tolerance\n",
    "        Repeat.\n",
    "        Returns learned CPT parameters.\n",
    "        \"\"\"\n",
    "        lls= []\n",
    "        prev_ll = -np.inf\n",
    "        for it in range(1, max_iters+1):\n",
    "            self.e_step()\n",
    "            self.m_step()\n",
    "            ll = self.log_likelihood()\n",
    "            lls.append(ll)\n",
    "            if verbose:\n",
    "                print(f\"Iter {it:3d}  ll = {ll:.6f}\")\n",
    "            if np.isfinite(prev_ll) and abs(ll - prev_ll) < tol:\n",
    "                if verbose:\n",
    "                    print(\"Converged (tol).\")\n",
    "                break\n",
    "            prev_ll = ll\n",
    "        \n",
    "        return {\n",
    "            'P_H_given_RY': self.P_H_given_RY,\n",
    "            'P_Pe_given_H': self.P_Pe_given_H,\n",
    "            'P_Sp_given_H': self.P_Sp_given_HPe,\n",
    "            'P_elem': self.P_elem\n",
    "        }\n",
    "\n",
    "    def predict_pollution_distribution(self, row):\n",
    "        \"\"\"\n",
    "        Compute P(Pollution | observed row)\n",
    "        Uses:\n",
    "            P(Pe | row) = sum_h P(Pe | H=h) * P(H=h | row)\n",
    "        \"\"\"\n",
    "        # compute posterior P(H | row)\n",
    "        p_h = self.compute_posterior_hidden_probs(row)\n",
    "        p_pe = (p_h[:, None] * self.P_Pe_given_H).sum(axis=0)\n",
    "        p_pe = p_pe / (p_pe.sum() + EPS)\n",
    "        return p_pe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ceca82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   1  ll = -44320.838619\n",
      "Iter   2  ll = -44276.788914\n",
      "Iter   3  ll = -44248.434020\n",
      "Iter   4  ll = -44226.911305\n",
      "Iter   5  ll = -44208.690389\n",
      "Iter   6  ll = -44192.219582\n",
      "Iter   7  ll = -44176.900131\n",
      "Iter   8  ll = -44162.576332\n",
      "Iter   9  ll = -44149.152279\n",
      "Iter  10  ll = -44136.429351\n",
      "Iter  11  ll = -44124.157400\n",
      "Iter  12  ll = -44112.140268\n",
      "Iter  13  ll = -44100.292554\n",
      "Iter  14  ll = -44088.643541\n",
      "Iter  15  ll = -44077.313832\n",
      "Iter  16  ll = -44066.480446\n",
      "Iter  17  ll = -44056.335703\n",
      "Iter  18  ll = -44047.044673\n",
      "Iter  19  ll = -44038.710570\n",
      "Iter  20  ll = -44031.358686\n",
      "Iter  21  ll = -44024.942901\n",
      "Iter  22  ll = -44019.368545\n",
      "Iter  23  ll = -44014.519773\n",
      "Iter  24  ll = -44010.281575\n",
      "Iter  25  ll = -44006.552496\n",
      "Iter  26  ll = -44003.249112\n",
      "Iter  27  ll = -44000.305333\n",
      "Iter  28  ll = -43997.669424\n",
      "Iter  29  ll = -43995.300596\n",
      "Iter  30  ll = -43993.166030\n",
      "Iter  31  ll = -43991.238603\n",
      "Iter  32  ll = -43989.495254\n",
      "Iter  33  ll = -43987.915861\n",
      "Iter  34  ll = -43986.482486\n",
      "Iter  35  ll = -43985.178884\n",
      "Iter  36  ll = -43983.990180\n",
      "Iter  37  ll = -43982.902673\n",
      "Iter  38  ll = -43981.903713\n",
      "Iter  39  ll = -43980.981627\n",
      "Iter  40  ll = -43980.125666\n",
      "Iter  41  ll = -43979.325966\n",
      "Iter  42  ll = -43978.573509\n",
      "Iter  43  ll = -43977.860078\n",
      "Iter  44  ll = -43977.178206\n",
      "Iter  45  ll = -43976.521126\n",
      "Iter  46  ll = -43975.882715\n",
      "Iter  47  ll = -43975.257428\n",
      "Iter  48  ll = -43974.640248\n",
      "Iter  49  ll = -43974.026626\n",
      "Iter  50  ll = -43973.412436\n",
      "Pollution bucket prediction accuracy: 0.687\n"
     ]
    }
   ],
   "source": [
    "cols_we_keep = [\n",
    "    'Nitrogen (% dw)_binned',\n",
    "    'Sulfur (% dw)_binned',\n",
    "    'Phosphorous (ppm dw)_binned',\n",
    "    'Lead (ppm dw)_binned',\n",
    "    'Copper (ppm dw)_binned',\n",
    "    'Chromium (ppm dw)_binned',\n",
    "    'Year of tissue collection_binned',\n",
    "    'Air pollution score_binned',\n",
    "    'Region_binned',\n",
    "    'Code for scientific name and authority in lookup table_binned'\n",
    "]\n",
    "df_clean = df[cols_we_keep].dropna().reset_index(drop=True)\n",
    "\n",
    "train_df, test_df = train_test_split(df_clean, test_size=0.2, random_state=42)\n",
    "# train em\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "model = BayesianNetworkEM(train_df,hidden_states=2, seed=0) \n",
    "\n",
    "model.run(max_iters=50, tol=1e-6)\n",
    "\n",
    "# eval on test set\n",
    "true_test_labels = test_df['Air pollution score_binned'].values\n",
    "poll_mapping = model.maps[model.pollution_col]\n",
    "predicted_labels = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    probs = model.predict_pollution_distribution(row)\n",
    "    # Pick most likely bucket\n",
    "    pred_idx = np.argmax(probs)\n",
    "    pred_label = model.rev[model.pollution_col][pred_idx]\n",
    "    predicted_labels.append(pred_label)\n",
    "\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "# accuracy score\n",
    "accuracy = np.mean(predicted_labels == true_test_labels)\n",
    "print(f\"Pollution bucket prediction accuracy: {accuracy:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
