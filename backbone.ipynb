{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6044d56f",
   "metadata": {},
   "source": [
    "# Lichen Likelihood Project: Expectation Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33cb1520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd5f9173",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'element_analysis.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m unpickled_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43melement_analysis.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/Projects/Cpp_Code/env/lib/python3.9/site-packages/pandas/io/pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/Cpp_Code/env/lib/python3.9/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'element_analysis.pkl'"
     ]
    }
   ],
   "source": [
    "unpickled_df = pd.read_pickle('element_analysis.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb8158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Nitrogen (% dw)_binned', 'Sulfur (% dw)_binned',\n",
       "       'Phosphorous (ppm dw)_binned', 'Lead (ppm dw)_binned',\n",
       "       'Copper (ppm dw)_binned', 'Chromium (ppm dw)_binned',\n",
       "       'Year of tissue collection_binned', 'Air pollution score_binned',\n",
       "       'Region_binned',\n",
       "       'Code for scientific name and authority in lookup table_binned'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print columns ending in \"binned\"\n",
    "unpickled_df.columns[unpickled_df.columns.str.endswith('binned')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a631ef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "df = unpickled_df.copy()\n",
    "def idx_map_from_series(s):\n",
    "    \"\"\"Return dict mapping original category -> index and reverse list\"\"\"\n",
    "    cats = list(pd.Categorical(s).categories)\n",
    "    mapping = {c:i for i,c in enumerate(cats)}\n",
    "    return mapping, cats\n",
    "\n",
    "def one_hot_index_from_value(mapping, val):\n",
    "    return mapping[val]\n",
    "\n",
    "def normalize_rows(mat, axis=-1):\n",
    "    s = mat.sum(axis=axis, keepdims=True)\n",
    "    s[s == 0] = 1.0\n",
    "    return mat / s\n",
    "\n",
    "def logsumexp(a, axis=None):\n",
    "    a_max = np.max(a, axis=axis, keepdims=True)\n",
    "    res = a_max + np.log(np.sum(np.exp(a - a_max), axis=axis, keepdims=True))\n",
    "    if axis is not None:\n",
    "        return np.squeeze(res, axis=axis)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BayesianNetworkEM:\n",
    "    def __init__(self, hidden_states=2, pollution_latent=True, seed=0):\n",
    "        \"\"\"\n",
    "        Initializes CPT tables with random values consistent with the BN structure.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.N = len(df)\n",
    "        self.H = hidden_states # need at least 2 latent classes \n",
    "        self.pollution_latent = pollution_latent\n",
    "        self.elements = [\n",
    "            'Nitrogen (% dw)_binned',\n",
    "            'Sulfur (% dw)_binned',\n",
    "            'Phosphorous (ppm dw)_binned',\n",
    "            'Lead (ppm dw)_binned',\n",
    "            'Copper (ppm dw)_binned',\n",
    "            'Chromium (ppm dw)_binned'\n",
    "        ]\n",
    "        self.year_col = 'Year of tissue collection_binned'\n",
    "        self.pollution_col = 'Air pollution score_binned'\n",
    "        self.region_col = 'Region_binned'\n",
    "        self.species_col = 'Code for scientific name and authority in lookup table_binned'\n",
    "        self.maps = {}\n",
    "        self.rev = {}\n",
    "        cols_for_map = self.elements + [self.year_col, self.pollution_col, self.region_col, self.species_col]\n",
    "        for col in cols_for_map:\n",
    "            mapping, cats = idx_map_from_series(self.df[col])\n",
    "            self.maps[col] = mapping\n",
    "            self.rev[col] = cats\n",
    "        self.K_region = len(self.rev[self.region_col])\n",
    "        self.K_year = len(self.rev[self.year_col])\n",
    "        self.K_poll = len(self.rev[self.pollution_col])\n",
    "        self.K_species = len(self.rev[self.species_col])\n",
    "        self.K_elem = {col: len(self.rev[col]) for col in self.elements}\n",
    "\n",
    "        # Initialize CPTs as probabilities, not log probs\n",
    "        self.initialize_random_CPTs()\n",
    "\n",
    "    def initialize_random_CPTs(self):\n",
    "            \"\"\"\n",
    "            Randomly initialize all CPTs:\n",
    "            Ensure each CPT is normalized over its conditional domain.\n",
    "            \"\"\"\n",
    "            # P(H | Region, Year)\n",
    "            self.P_H_given_RY = np.random.rand(self.K_region, self.K_year, self.H)\n",
    "            self.P_H_given_RY = normalize_rows(self.P_H_given_RY, axis=-1)\n",
    "            # P(Pe | H)\n",
    "            self.P_Pe_given_H = np.random.rand(self.H, self.K_poll)\n",
    "            self.P_Pe_given_H = normalize_rows(self.P_Pe_given_H, axis=-1)\n",
    "            # P(Sp | H, Pe)\n",
    "            self.P_Sp_given_HP = np.random.rand(self.H, self.K_poll, self.K_species)\n",
    "            self.P_Sp_given_HP = normalize_rows(self.P_Sp_given_HP, axis=-1)\n",
    "            # For each element: P(Elem | Sp, Pe)\n",
    "            self.P_elem_given_SP = {}\n",
    "            for col in self.elements:\n",
    "                self.P_elem_given_SP[col] = np.random.rand(self.K_species, self.K_poll, self.K_elem[col])\n",
    "                self.P_elem_given_SP[col] = normalize_rows(self.P_elem_given_SP[col], axis=-1)\n",
    "\n",
    "    def e_step(self):\n",
    "        \"\"\"\n",
    "        Performs the E-step\n",
    "            For each sample n:\n",
    "                Compute posterior responsibilities\n",
    "            Store in self.gamma with shape [num_samples, num_hidden_states]\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def compute_posterior_hidden_probs(self, sample_row):\n",
    "        \"\"\"\n",
    "        Compute P(H=h | observed sample features) for a single sample\n",
    "        Returns a vector normalized over hidden states\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def m_step(self):\n",
    "        \"\"\"\n",
    "        Performs the M-step:\n",
    "            Update CPTs:\n",
    "                - P(H | Region, FieldDate)\n",
    "                - P(Pollution | H)\n",
    "                - P(Species | H, Pollution)\n",
    "                - P(Element_i_bucket | Species, Pollution) for each element node\n",
    "        Uses expected counts\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def update_hidden_CPT(self):\n",
    "        \"\"\"\n",
    "        Update P(H | Region, FieldDate)\n",
    "        Accumulate weighted counts per condition using responsibilities calculated above\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def update_pollution_CPT(self):\n",
    "        \"\"\"\n",
    "        Update P(Pollution | H)\n",
    "        Pollution is observed, responsibilities provides the weighting.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def update_species_CPT(self):\n",
    "        \"\"\"\n",
    "        Update: P(Species | H, Pollution)\n",
    "        Species is observed, conditioned on hidden state and pollution bucket(S)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def update_element_bucket_CPTs(self):\n",
    "        \"\"\"\n",
    "        For each tissue element bucket node:\n",
    "            Update: P(ElementBucket | Species, Pollution)\n",
    "        Summed over hidden responsibilities since element buckets have no H parent\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def log_likelihood(self):\n",
    "        \"\"\"\n",
    "        Compute the complete-data log likelihood using current CPTs and responsibilities\n",
    "        Used for eval\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def run(self, max_iters=50, tol=1e-6):\n",
    "        \"\"\"\n",
    "        Full EM loop:\n",
    "        initialize_random_CPTs()\n",
    "        e_step()\n",
    "        m_step()\n",
    "        compute log likelihood and check tolerance\n",
    "        Repeat.\n",
    "        Returns learned CPT parameters.\n",
    "        \"\"\"\n",
    "    def predict_pollution_distribution(self, row):\n",
    "        \"\"\"\n",
    "        Compute P(Pollution | observed row)\n",
    "        using:\n",
    "            P(P | H) * P(H | row)\n",
    "\n",
    "        return numpy array of num_pollution_values\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ceca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnem= BayesianNetworkEM(hidden_states=2, pollution_latent=True, seed=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
